#!/bin/bash

# ==============================================================================
#  Get FASTQ files from the ENA database
# ==============================================================================
ver="2.1.0"

# --- Source common settings and functions -------------------------------------
# NOTE: 'realpath' expands symlinks by default. Thus, $xpath is always the real
#       installation path, even when this script is called by a symlink!
xpath="$(dirname "$(realpath "$0")")"
source "${xpath}"/workers/x.funx.sh
source "${xpath}"/workers/progress_funx.sh

# --- Help message -------------------------------------------------------------

read -d '' _help_getfastq << EOM || true
getFASTQ allows easy downloading of FASTQ files (typically NGS read data) from
the European Nucleotide Archive (ENA) database via HTTP connection. Each target
file is specified by an entry of this type

    wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRRxxx/SRRxxxxxx/SRRxxxxxx.fastq.gz

within an index file (referred to as TARGETS) passed as input to getFASTQ. This
format complies with the download script generated by ENA Browser upon manual
file selection ("Get download script" button). By default, a sequential download
queue is created to retrieve all the targets, running the whole process with
persistence (i.e., ignoring the hangup signal HUP) and in the background. Also,
FASTQ files are checked for integrity after download by using MD5 hashes. Option
flags are provided for parallel downloading, foreground execution, and integrity
check suppression.

Usage:
  getfastq [-h | --help] [-v | --version]
  getfastq -p | --progress[-complete] [TARGETS]
  getfastq -k | --kill
  getfastq -u | --urls PRJ_ID [> TARGETS]
  getfastq [-q | --quiet] [-w | --workflow] [-m | --multi] [--no-checksum] TARGETS

Positional options:
  -h | --help      Shows this help.
  -v | --version   Shows script's version.
  -p | --progress  Shows download progress by scraping ALL the getFASTQ log
                   files found in TARGETS directory (including those currently
                   growing). If TARGETS is not provided, it defaults to \$PWD.
                   The '--progress-complete' variant option just returns 'true'
                   when all the scheduled FASTQ files have been correctly
                   downloaded, or 'false' otherwise.
  -k | --kill      Gracefully (-15) kills all the 'wget' processes currently
                   running and started by the current user.
  -u | --urls      Fetches from ENA the list of FTP download URLs for the full
                   set of Runs (i.e., FASTQ files) making up a given BioProject.
                   This job is never run in the background and, by default, the
                   'wget' output lines are just sent to stdout. Other possible
                   messages are sent to stderr.
  PRJ_ID           Placed right after the '-u | --urls' flag, is any valid
                   BioProject accession number as issued by the INSDC and used
                   in ENA to identify the Study whose Runs are to be retrieved
                   (e.g., "PRJNA141411"). GEO Series IDs (i.e., "GSE29580") are
                   also allowed and automatically converted to ENA/INSDC
                   BioProject IDs beforehand.
  -q | --quiet     Disables verbose on-screen logging.
  -w | --workflow  Makes processes run in the foreground for use in pipelines.
  -m | --multi     Multi process option. A separate download process will be
                   instantiated for each target FASTQ file at once, resulting in
                   a parallel download of all the TARGETS. While the default
                   behavior is the sequential download of the individual FASTQs,
                   using '-m' option can result in a much faster global download
                   process, especially in the case of broadband internet
                   connections.
  --no-checksum    Attempts each download once and ignores the checksum.
  TARGETS          Path to the text file containing the 'wgets' to be scheduled,
                   in the format provided by ENA Browser. Placed right after
                   '-p' option, it is the (file or folder) path where to look
                   for getFASTQ progress logs.

Additional Notes:
  . Target addresses need to be converted to HTTP because of the limitations on
    FTP imposed by UniTo. Luckily, this can be done simply by replacing 'ftp'
    with 'http' in each URL to wget, thanks to the great versatility of the ENA
    Browser.
  . Use
      watch getfastq -p
      watch -cn 0.5 'getfastq -p [TARGETS]'
    to follow the growth of a log file in real time.
  . getfastq --progress-complete option can be very useful for checking the
    completeness of the FASTQ fileset before continuing the pipeline when
    working in workflow mode. E.g.:
      if ! getfastq --progress-complete; then exit 1; fi
  . While the 'getfastq -k' option tries to gracefully kill ALL the currently
    active 'wget' processes started by \$USER, you may wish to selectively kill
    just some of them (possibly forcefully) after you retrieved their IDs
    through 'pgrep -l -u "\$USER"'.
  . To download an entire BioProject (aka Study or Series) you need a two-step
    procedure. E.g.:
      getfastq --urls PRJNA141411 > PRJNA141411_wgets.sh
      getfastq PRJNA141411_wgets.sh 
EOM

# --- Argument parsing and validity check --------------------------------------

# Default options
verbose=true
pipeline=false
sequential=true
integrity=true

# Flag Regex Pattern (FRP)
frp="^-{1,2}[a-zA-Z0-9-]+$"

# Argument check: options
while [[ $# -gt 0 ]]; do
    if [[ "$1" =~ $frp ]]; then
        case "$1" in
            -h | --help)
                printf "%s\n" "$_help_getfastq"
                exit 0
            ;;
            -v | --version)
                _print_ver "get FASTQ" "${ver}" "Hedmad & FeAR"
                exit 0
            ;;
            -p | --progress | --progress-complete)
                if [[ $1 == "--progress-complete" ]]; then
                    ok=$(_progress_getfastq "${2:-.}" | grep -F "Completed:" \
                        | sed -E 's|.*\[([0-9]+)\/.*|\1|' || [[ $? == 1 ]])
                    tot=$(_progress_getfastq "${2:-.}" | grep -F "Completed:" \
                        | sed -E 's|.*\/([0-9]+)\]$|\1|' || [[ $? == 1 ]])
                    if [[ $ok -eq $tot ]]; then
                        # return TRUE
                        exit 0
                    else
                        # return FALSE (without triggering the ERR trap)
                        exit 11
                    fi
                else
                    _progress_getfastq "${2:-.}"
                    exit 0
                fi
            ;;
            -k | --kill)
                _gracefully_kill "wget"
                exit 0
            ;;
            -u | --urls)
                if [[ -n "${2:-}" ]]; then
                    # BioProject Accession ID Regex Patterns
                    ena_rgx="^PRJ(E|D|N)[A-Z][0-9]+$"
                    geo_rgx="^GSE[0-9]+$"
                    # If GEO ID, then convert to ENA/INSDC
                    if [[ $2 =~ $geo_rgx ]]; then
                        ena_accession_id=$(_geo2ena_id $2)
                        if [[ $ena_accession_id == NA ]]; then
                            eprintf "Cannot convert GEO Series ID to ENA alias...\n"
                            exit 21
                        fi
                        eprintf "GEO Series ID detected and converted to "\
                            "the ENA/INSDC BioProject ID: " \
                            "$2 --> ${ena_accession_id}\n"
                    elif [[ $2 =~ $ena_rgx ]]; then
                        ena_accession_id=$2
                        eprintf "ENA/INSDC BioProject ID detected: " \
                            "${ena_accession_id}\n"
                    else
                        eprintf "Invalid BioProject ID: ${2}\n" \
                            "Unknown format.\n"
                        exit 6
                    fi
                    # Get download URLs from ENA 
                    _fetch_ena_project_json "$ena_accession_id" | \
                        _extract_download_urls
                    exit 0
                else
                    eprintf "Missing value for PRJ_ID.\n" \
                        "Use '--help' or '-h' to see the expected syntax.\n"
                    exit 5
                fi
            ;;
            -q | --quiet)
                verbose=false
                shift
            ;;
            -w | --workflow)
                pipeline=true
                shift
            ;;
            -m | --multi)
                sequential=false
                shift
            ;;
            --no-checksum)
                integrity=false
                shift
            ;;
            *)
                _print_bad_flag $1
                exit 4
            ;;
        esac
    else
        # The first non-FRP sequence is taken as the TARGETS argument
        target_file="$(realpath "$1")"
        shift
    fi
done

# Argument check: target file
_check_target "file" "${target_file:-}"

# --- Main program -------------------------------------------------------------

# Verbose on-screen logging
header="getFASTQ :: NGS Read Retriever :: ver.${ver}"
if $verbose; then
    printf "${header}\n\n"
    echo "========================"
    if $sequential; then
        echo "| Sequential Job Queue |"
    else
        echo "|  Parallel Job Queue  |"
    fi
    echo "========================"

    counter=1
    while IFS= read -r line
    do
        # Bash-native string substitution syntax to change FTP into HTTP
        fastq_name="$(basename "$line")"
        fastq_address="$(dirname ${line/wget* ftp:/http:})"
        echo
        echo "[${counter}]"
        echo "Downloading: $fastq_name"
        echo "From       : $fastq_address"
        ((counter++))
    done < "$target_file"
fi

# Make a temporary copy of TARGETS file, where:
# - FTP is replaced by HTTP;
# - wget's -P option is added to specify the destination;
# - the progress bar is forced even if the output is not a TTY (see 'man wget');
# - possible spaces in paths are escaped to avoid issues in the next part.
target_file_tmp="$(mktemp)"
target_dir="$(dirname "$target_file")"
sed "s|ftp:|--progress=bar:force -P ${target_dir//" "/"\\\ "} http:|g" \
    "$target_file" > "$target_file_tmp"

# This function is triggered by '_process_series' and takes a single wget-FASTQ
# target, along with its expected MD5 hash as fetched from ENA, in order to
# (i) perform the actual download of the FASTQ file, (ii) check its integrity by
# MD5 checksum, (iii) retry the download three times if checksum fails.
function _process_sample {

    local eval_str="$1"
    local target="$(basename "$eval_str")"
    local checksum="$2"
    local attempt=1

    if $integrity; then
        while true; do
            printf "%b" "Spawning download worker for $target " \
                "with checksum $checksum (attempt ${attempt})\n"
            # Download the FASTQ here!
            bash -c "$eval_str"

            local local_hash=$(cat "${target_dir}/${target}" | md5sum | \
                cut -d' ' -f1)
            printf "Computed hash: $local_hash - "
            if [[ $checksum == $local_hash ]]; then
                printf "Success!\n"
                return
            else
                printf "FAILURE! Deleting corrupt file...\n"
                rm "${target_dir}/${target}"
                if [[ $attempt -lt 3 ]]; then
                    printf "File was corrupted in transit. Trying again.\n\n"
                    ((attempt++))
                else
                    printf "Unable to download $target - corrupted checksum\n"
                    return
                fi
            fi
        done
    else
        printf "Spawning download worker for ${target}\n"
        bash -c "$eval_str"
    fi
}

# This function takes a file with a list of wget-FASTQ targets and, for each one
# of them, (i) makes a log file, (ii) retrieves from ENA the expected MD5 hash,
# (iii) prepares sample download by calling '_process_sample' function with the
# correct hold-on setting, depending on the selected download mode.
function _process_series {

    local target_file_tmp="$1"

    while IFS= read -r line
    do
        # Set the log files (using sample IDs)
        local sample_id="$(basename "$line" | sed -E "s/(\.fastq|\.gz)//g")"
        local log_file="${target_dir}/Z_getFASTQ_${sample_id}_$(_tstamp).log"
        printf "%b-- $(_tstamp) -- ${header}\n" > "$log_file"
        
        # Remove possible PE read suffix and retrieve the real MD5 from ENA
        local ena_id=$(echo $sample_id | cut -d'_' -f1)
        local checksums=$(_fetch_ena_sample_hash $ena_id)
        if [[ $sample_id =~ ^.*_R?2$ ]]; then
            local checksum=$(echo $checksums | cut -d';' -f2)
        else
            local checksum=$(echo $checksums | cut -d';' -f1)
        fi

        # HOLD-ON STATEMENT for "parallel" mode
        if $sequential; then
            _process_sample "$line" "$checksum" >> "$log_file" 2>&1
        else
            # Mind the final '&' to make the job parallel when in workflow mode
            _hold_on "$log_file" _process_sample "$line" "$checksum" &
        fi
    done < "$target_file_tmp"
}

# HOLD-ON STATEMENT for "sequential" mode
if $sequential; then
    _hold_on /dev/null _process_series "$target_file_tmp"
else
    _process_series "$target_file_tmp"
    # Wait for background processes to finish when in 'workflow' mode
    $pipeline && wait
fi
